{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Python'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from Python.config import config\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import Python.WRIC_preprocessing as wric\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1_metadata, R2_metadata, df_room1, df_room2 = wric.preprocess_WRIC_file(\"C:\\Documents\\WRIC_example_data\\Results_1m_copy_anonymised.txt\", code=\"id+comment\", path_to_save=None) #path_to_save=\"C:\\Documents\\WRIC_example_data\"\n",
    "display(df_room1)\n",
    "\n",
    "# do not include discrepancy check standard and explain that without calibration gases no realistic values (only do for actual person values)\n",
    "#check_discrepancies(df_room2, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_WRIC_files('id.csv', 'upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./example_data/note.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "if not lines[-1].endswith('\\n'):\n",
    "    with open('./example_data/note.txt', 'a') as file:\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting protocol for all people "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"D:/Simon_CIRCLE/WRIC/processed\"\n",
    "protocol_dict = {\"normal\" : 0, \"sleep\" : 1, \"eat\" : 2, \"active\" : 3, \"ree\" : 4}\n",
    "\n",
    "# choose the protocol you want (takes first) and number, if there are multiple specify the occurence (@Nina: start counting at 1!)\n",
    "def tmp_func_name(folder_path, protocol, occurence = 1, add_start = 0, add_end = 0):\n",
    "    # add_start, add_end in minutes\n",
    "    wric_files = [f for f in os.listdir(folder_path) if f.endswith(\"_data.csv\")]\n",
    "    try:\n",
    "        protocol_num = protocol_dict[protocol]\n",
    "    except:\n",
    "        print(\"ERROR: Please provide a valid protocol instance: normal, sleep, eat, active, ree\")\n",
    "        return\n",
    "    for file in wric_files:\n",
    "        print(file)\n",
    "        df = pd.read_csv(folder_path +\"/\" + file)\n",
    "        \n",
    "        is_protocol = df[\"protocol\"] == protocol_num\n",
    "        transitions = is_protocol & (~is_protocol.shift(fill_value=False))\n",
    "        occurence_index = transitions[transitions].index[occurence-1]\n",
    "        start_datetime = df.loc[occurence_index, \"datetime\"]\n",
    "        \n",
    "        end_transitions = (~is_protocol) & is_protocol.shift(fill_value=False)\n",
    "        try:\n",
    "            end_index = end_transitions[end_transitions].index[occurence -1]\n",
    "            end_datetime = df.loc[end_index, \"datetime\"]\n",
    "        except IndexError:\n",
    "            end_datetime = None\n",
    "        \n",
    "        start = pd.to_datetime(start_datetime) - pd.Timedelta(minutes=add_start)\n",
    "        end = pd.to_datetime(end_datetime) + pd.Timedelta(minutes=add_end)\n",
    "        \n",
    "        # Check if start/end is earlier/later than the earliest/latest datetime in the DataFrame\n",
    "        if start < pd.to_datetime(df[\"datetime\"].min()):\n",
    "            print(f\"Warning: Start time {start} is earlier than the earliest data point. Using {df['datetime'].min()} instead.\")\n",
    "            start = pd.to_datetime(df[\"datetime\"].min())\n",
    "        if end > pd.to_datetime(df[\"datetime\"].max()):\n",
    "            print(f\"Warning: End time {end} is later than the latest data point. Using {df['datetime'].max()} instead.\")\n",
    "            end = pd.to_datetime(df[\"datetime\"].max())\n",
    "          \n",
    "        df = wric.cut_rows(df, start, end)\n",
    "        display(df.head())\n",
    "        if (set(df[\"protocol\"].unique()) != {0, protocol_num}):\n",
    "            print(\"WARNING: The time you specified includes other protocols than normal and\", protocol, \"Be aware of that for your analysis!\")\n",
    "            print(start, end)\n",
    "            print(pd.isna(start), pd.isna(end))\n",
    "        df.drop(columns=[\"relative_time[min]\"])\n",
    "           \n",
    "        df = wric.add_relative_time(df)\n",
    "        \n",
    "        #display(df.head())\n",
    "        \n",
    "        # add/change reltive time\n",
    "        \n",
    "        #save\n",
    "        \n",
    "        \n",
    "tmp_func_name(folder_path, \"sleep\", occurence=2)       \n",
    "\n",
    "# output warning if changed to other protocol_num than 0 (interference)\n",
    "# extrcat and save them as new DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"D:/Simon_CIRCLE/WRIC/01JJ_wric1min_v1_treat0.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WRIC_preprocessing as wric\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder1 = \"/media/nina/SUNSHINE/Simon_CIRCLE/WRIC/Notes_Processed\"\n",
    "files_without_note = [file for file in os.listdir(folder1)]\n",
    "\n",
    "print(files_without_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wric_dict = {'01JJ_wric1min_v1_treat0.txt' : '01JJ_wric1min_v1_note_treat0.txt', \n",
    "    '01JJ_wric1min_v2_treat1.txt' : '01JJ_wric1min_v2_note_treat1.txt', \n",
    "    '02LK_v2_treat0_wric1min_04HH_v2_treat1.txt' : '02LK_v2_treat0_wric1min_04HH_v2_treat1_note.txt', \n",
    "    '02LK_wric1min_v1_treat1.txt' : '02LK_wric1min_v1_note_treat1.txt', \n",
    "    '03HA_v1_treat1_wric1min_04HH_v1_treat0.txt' : '03HA_v1_treat1_wric1min_04HH_v1_treat0_note.txt', \n",
    "    '03HA_wric1min_v2_treat0.txt' : '03HA_wric1min_v2_note_treat0.txt', \n",
    "    '05PM_wric1min_v1_treat0.txt' : '05PM_wric1min_v1_note_treat0.txt', \n",
    "    '05PM_wric1min_v2_treat1.txt' : '05PM_wric1min_v2_note_treat1.txt', \n",
    "    '06ML_v2_treat1_wric1min_09NQ_v1_treat1.txt' : '06ML_v2_treat1_wric1min_09NQ_v1_treat1_note.txt', \n",
    "    '06ML_wric1min_v1_treat0.txt' : '06ML_wric1min_v1_note_treat0.txt', \n",
    "    '07AB_v1_treat1_wric1min_08MG_v2_treat0.txt' : '07AB_v1_treat1_wric1min_08MG_v2_treat0_note.txt', \n",
    "    '07AB_wric1min_v2_treat0.txt' : '07AB_wric1min_v2_note_treat0.txt', \n",
    "    '08MG_wric1min_v1_treat1.txt' : '08MG_wric1min_v1_note_treat1.txt', \n",
    "    '09NQ_wric1min_v2_treat0.txt' : '09NQ_wric1min_v2_note_treat0.txt', \n",
    "    '10JK_wric1min_v1_treat0.txt' : '10JK_wric1min_v1_note_treat0.txt', \n",
    "    '10JK_wric1min_v2_treat1.txt' : '10JK_wric1min_v2_note_treat1.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [\"01JJ\", \"02LK\", \"03HA\", \"04HH\", \"05PM\", \"06ML\", \"07AB\", \"08MG\", \"09NQ\", \"10JK\"]\n",
    "treatments = [0, 1]\n",
    "base_folder = \"/media/nina/SUNSHINE/Simon_CIRCLE/WRIC/\"\n",
    "note_base_folder = \"/media/nina/SUNSHINE/Simon_CIRCLE/WRIC/Notes_Processed/\"\n",
    "path_to_save = \"/media/nina/SUNSHINE/Simon_CIRCLE/WRIC/processed\"\n",
    "\n",
    "for filepath, notepath in wric_dict.items():\n",
    "    print(filepath, notepath)\n",
    "    R1_metadata, R2_metadata, df_room1, df_room2 = wric.preprocess_WRIC_file(base_folder+filepath, code=\"id+comment\", path_to_save=path_to_save, notefilepath=note_base_folder+notepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for Note Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1_metadata, R2_metadata, df_room1, df_room2 = wric.preprocess_WRIC_file(\"C:/Documents/WRIC_example_data/Results_1m_copy_anonymised.txt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the note file into a pandas Dataframe\n",
    "notes_path = \"C:/Documents/WRIC_example_data/Main_note_yyyymmddxxxx.txt\"\n",
    "notes_content = wric.open_file(notes_path)\n",
    "\n",
    "lines = [line.strip().split('\\t') for line in notes_content[2:]]\n",
    "\n",
    "df_note = pd.DataFrame(lines[2:], columns=lines[0])\n",
    "df_note = df_note.dropna()\n",
    "\n",
    "# combine to datetime\n",
    "df_note['datetime'] = pd.to_datetime(df_note['Date'] + ' ' + df_note['Time'], format='%m/%d/%y %H:%M:%S')\n",
    "df_note = df_note.drop(columns=['Date', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_note.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dict_protocol, participant, datetime, value):\n",
    "    if participant is not None:\n",
    "        dict_protocol[participant][datetime] = value\n",
    "    else:\n",
    "        dict_protocol[1][datetime] = value\n",
    "        dict_protocol[2][datetime] = value\n",
    "    return dict_protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_protocol(df, protocol_list):\n",
    "    \"\"\"HELPER FUNCTION _ DO NOT USE! Update the protocol column in the given DataFrame based on the provided protocol list.\"\"\"\n",
    "    current_protocol = 0\n",
    "    current_index = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # While there are more timestamps and the current row's datetime is greater than or equal to the timestamp\n",
    "        while (current_index < len(protocol_list) and \n",
    "               row['datetime'] >= protocol_list[current_index][0]):\n",
    "            current_protocol = protocol_list[current_index][1]  # Update current protocol\n",
    "            current_index += 1  # Move to the next timestamp\n",
    "\n",
    "        df.at[index, 'protocol'] = current_protocol\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "keywords_dict = {\n",
    "    'sleeping': ([\"seng\", \"sleeping\", \"bed\", \"sove\", \"soeve\"], 1),\n",
    "    'eating': ([[\"start\", \"begin\", \"began\"],[\"maaltid\", \"måltid\", \"eat\", \"meal\", \"food\", \"spis\", \"maal\", \"måd\", \"mad\", \"frokost\", \"morgenmad\", \"middag\", \"snack\", \"aftensmad\"]], 2),\n",
    "    'stop_sleeping' : ([\"vaagen\", \"vågen\", \"vaekket\", \"væk\", \"awake\", \"wake\", \"woken\"], 0),\n",
    "    'stop_anything': ([\"faerdig\", \"færdig\", \"stop\", \"end\", \"finished\", \"slut\"], 0),\n",
    "    'activity': ([[\"start\", \"begin\", \"began\"], [\"step\", \"exercise\", \"physicial activity\", \"active\", \"motion\", \"aktiv\"]], 3),\n",
    "    'ree_start': ([[\"start\", \"begin\", \"began\"], [\"REE\"]], 4),\n",
    "    # TODO: Cut based on start and end extracted from notes\n",
    "    'end': ([\"ud\", \"exit\", \"out\"], 0), #maybe as added safety check, check that it is the last/first note for that participant\n",
    "    'start': ([\"ind i kammer\", \"enter\", \"ind\"], 0)\n",
    "}\n",
    "\n",
    "time_pattern = r\"([0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5]\\d\"\n",
    "dict_protocol = {1:{}, 2:{}}\n",
    "for index, row in df_note.iterrows():\n",
    "    participant = None\n",
    "    if row[\"Comment\"].startswith(\"1\"):\n",
    "        participant = 1\n",
    "    elif row[\"Comment\"].startswith(\"2\"):\n",
    "        participant = 2\n",
    "    for category, (keywords, value) in keywords_dict.items():\n",
    "        if isinstance(keywords[0], list):\n",
    "            # Multi-group check: at least one keyword from each sublist must match\n",
    "            if all(any(word.lower() in row['Comment'].lower() for word in group) for group in keywords):\n",
    "                print(\"multi match\", row[\"Comment\"])\n",
    "                # check if a different timestamp is written in the message and save the value there \n",
    "                # only checks first time stamp and only in format 6:45 or 06:45\n",
    "                match = re.search(time_pattern, row['Comment'])\n",
    "                if match:\n",
    "                    time_str = match[0]\n",
    "                    date_str = row['datetime'].date()\n",
    "                    new_datetime = pd.Timestamp(datetime.combine(date_str, datetime.strptime(time_str, \"%H:%M\").time()))\n",
    "                    dict_protocol = save_dict(dict_protocol, participant, new_datetime, value)\n",
    "                else:\n",
    "                    dict_protocol = save_dict(dict_protocol, participant, row[\"datetime\"], value)\n",
    "        else:\n",
    "            # Single-group check: only one keyword needs to match\n",
    "            if any(word.lower() in row['Comment'].lower() for word in keywords):\n",
    "                match = re.search(time_pattern, row['Comment'])\n",
    "                if match:\n",
    "                    time_str = match[0]\n",
    "                    date_str = row['datetime'].date()\n",
    "                    new_datetime = pd.Timestamp(datetime.combine(date_str, datetime.strptime(time_str, \"%H:%M\").time()))\n",
    "                    dict_protocol = save_dict(dict_protocol, participant, new_datetime, value)\n",
    "                else:\n",
    "                    dict_protocol = save_dict(dict_protocol, participant, row[\"datetime\"], value)\n",
    "            \n",
    "protocol_list_1 = sorted(dict_protocol[1].items())\n",
    "protocol_list_2 = sorted(dict_protocol[2].items())\n",
    "\n",
    "update_protocol(df_room1, protocol_list_1)\n",
    "update_protocol(df_room2, protocol_list_2)\n",
    "\n",
    "print(protocol_list_1)\n",
    "print(protocol_list_2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_room1)\n",
    "df_room1.to_csv(\"./tmp/df_room1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"bæsrefg 9:33 dlsfgkbnwee 10:54 wkregf\"\n",
    "b = \"kljbfdavg wergoi\"\n",
    "pattern = r\"\\b([01]?\\d|2[0-3]):[0-5]\\d\\b\"\n",
    "match = re.search(pattern, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bool(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
